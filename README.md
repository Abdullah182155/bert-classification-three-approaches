#  BERT-Based Text Classification: Three Approaches Comparison


## Overview

This project demonstrates **three different approaches** to text classification using pre-trained BERT models **without fine-tuning**. The goal is to explore how transformer-based encoder models can be leveraged as static feature extractors for downstream classification tasks.

### Key Features

- ‚úÖ **No Fine-tuning Required**: BERT weights remain frozen throughout all approaches
- ‚úÖ **Three Distinct Methods**: Compare different strategies for using BERT embeddings
- ‚úÖ **Complete Evaluation**: Detailed classification reports with precision, recall, and F1-scores
- ‚úÖ **Interactive Demo**: Web-based Gradio interface for real-time predictions
- ‚úÖ **Production-Ready Code**: Well-structured notebook with clear documentation

---

## Three Approaches Explained

### 1Ô∏è‚É£ **BERT + Trainable Classification Head**
- Extract embeddings from frozen BERT model
- Add custom neural network layers on top of [CLS] token
- Train only the classification head
- **Best for**: Learning task-specific patterns from your data

### 2Ô∏è‚É£ **BERT as Feature Extractor + Traditional ML**
- Use BERT to generate sentence embeddings
- Train classical ML classifier (Logistic Regression, SVM, etc.)
- Leverage proven ML algorithms with modern representations
- **Best for**: Quick experimentation and interpretability

### 3Ô∏è‚É£ **Zero-Shot Classification via Cosine Similarity**
- Embed both input text and label descriptions
- Compute similarity scores between text and each label
- Assign class with highest similarity (no training required!)
- **Best for**: Scenarios with limited labeled data or rapid prototyping

---

## üìä Dataset

This project uses the **Auditor Review Dataset** from Hugging Face:
- **Source**: `rajistics/auditor_review`
- **Task**: Multi-class text classification
- **Features**: Sentences from auditor reviews with sentiment/classification labels

The code is modular and can be easily adapted to any text classification dataset.


### Install Dependencies
```bash
pip install transformers datasets torch scikit-learn gradio pandas numpy
```

## Quick Start



### 3. Execute Cells Sequentially
The notebook is organized into clear sections:
- **Setup & Data Loading** (Cells 1-4)
- **BERT Model Loading** (Cells 5-7)
- **Approach 1: Classification Head** (Cell 8)
- **Approach 2: Traditional ML** (Cell 9)
- **Approach 3: Cosine Similarity** (Cell 10)
- **Comparison & Results** (Cell 11)
- **Interactive Gradio App** (Cell 12)

### 4. Launch the Web Interface
The final cell launches an interactive Gradio app where you can:
- Input custom text
- Select any of the three approaches
- Get real-time predictions with confidence scores

---




## üîß Customization

### Use Your Own Dataset


In Cell 5, replace `bert-base-uncased` with any encoder model:
```python
model_name = "distilbert-base-uncased"  # Faster, smaller
model_name = "roberta-base"             # Often better performance
model_name = "albert-base-v2"           # Parameter-efficient
```

### Adjust Label Descriptions (Approach 3)

In Cell 10, customize the label descriptions for better zero-shot performance:
```python
label_descriptions = {
    0: "This text expresses negative sentiment about...",
    1: "This text is neutral and factual about...",
    2: "This text expresses positive sentiment about..."
}
```

---

## üí° Key Insights

### When to Use Each Approach?

**Approach 1 (Classification Head)**
- ‚úÖ You have sufficient labeled training data
- ‚úÖ You want the best possible accuracy
- ‚úÖ You can afford training time

**Approach 2 (Traditional ML)**
- ‚úÖ You want fast experimentation
- ‚úÖ You need interpretable models
- ‚úÖ You have limited compute resources

**Approach 3 (Cosine Similarity)**
- ‚úÖ You have very little labeled data
- ‚úÖ You need predictions immediately (zero-shot)
- ‚úÖ Your labels are well-described in natural language

---

## üìö Technical Details

### BERT Embedding Extraction
- Uses the **[CLS] token** from the final hidden state
- Embeddings are **768-dimensional** (for base models)
- Batch processing for efficiency

### Training Configuration
- **Optimizer**: Adam (lr=0.001)
- **Loss Function**: CrossEntropyLoss
- **Batch Size**: 32
- **Epochs**: 10 (for Approach 1)

### Hardware Requirements
- **Minimum**: 8GB RAM, CPU only (slower)
- **Recommended**: 16GB RAM, CUDA GPU
- **Optimal**: 32GB RAM, GPU with 8GB+ VRAM

---


## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Hugging Face** for the Transformers library and datasets
- **Google AI** for BERT pre-trained models
- **Gradio** for the interactive interface framework
- **scikit-learn** for evaluation metrics and ML algorithms

---



## üìñ Citation

If you use this code in your research, please cite:

```bibtex
@software{bert_classification_three_approaches,
  author = {Your Name},
  title = {BERT-Based Text Classification: Three Approaches Comparison},
  year = {2024},
  url = {https://github.com/yourusername/bert-classification-three-approaches}
}
```

---

## üìù Changelog

### Version 1.0.0 (2024-12-09)
- ‚ú® Initial release
- üéØ Three classification approaches implemented
- üåê Gradio web interface
- üìä Comprehensive evaluation metrics

---

<div align="center">
  
**Made with ‚ù§Ô∏è by [Abdullah Ashraf]**



</div>
